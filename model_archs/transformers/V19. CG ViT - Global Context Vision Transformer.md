---
author:
  - A
  - B
submission:
  - PMIR
  - B
year: "2024"
file: 
related: 
tags:
  - Test-Time-Learning
review date: 2025-01-03
---
# <font color="#31859b">Summary</font> - 영어 원문 추가 필요

## 1-Line

```
GC ViT는 전역적 컨텍스트 자기 주의 모듈을 사용하여 장단거리 공간 상호작용을 효과적으로 모델링하는 새로운 비전 트랜스포머 아키텍처
```
## 3-Line

```
GC ViT는 전역적 컨텍스트와 지역적 자기 주의를 결합하여 효율적으로 공간 상호작용을 모델링
```

```
수정된 퓨즈드 인버티드 레지듀얼 블록을 도입하여 ViT의 귀납적 편향 부족 문제를 해결
```

```
ImageNet-1K, MS COCO, ADE20K 등 다양한 비전 태스크에서 SOTA 성능을 달성
```
## 5-Line

```

```

```

```

```

```

```
복잡한 어텐션 마스크나 윈도우 시프팅과 같은 비용이 많이 드는 연산이 필요하지 않음
```

```
51M, 90M, 201M 파라미터 버전으로 각각 84.3%, 85.0%, 85.7%의 Top-1 정확도를 달성
```


# <font color="#7030a0">Paper Review</font>

## Abstract / Motivation

- CNN은 장거리 의존성을 잘 포착하지 못하는 한계 ["In essence, the self-attention mechanism in ViT allows for learning more uniform short and long-range information in comparison to CNN" - Page 1, Introduction]

- 기존 ViT는 단일 아키텍처와 자기 주의의 제곱 복잡도로 인해 고해상도 이미지에 적용하기 어려움 ["the monolithic architecture of ViT and quadratic computational complexity of self-attention baffle their swift application to high resolution images" - Page 1-2, Introduction]

## Background

Swin Transformer와 같은 이전 연구들은 지역 윈도우에서 자기 주의를 계산하고 윈도우 시프팅으로 영역 간 상호작용을 모델링

## Targeting Problems

- 지역 윈도우의 제한된 수용 영역으로 인해 장거리 정보 포착이 어려움 ["the limited receptive field of local windows challenges the capability of self-attention to capture long-range information" - Page 2, Introduction]

- 윈도우 시프팅은 각 윈도우 주변의 작은 이웃만 커버 ["window-connection schemes such as shifting only cover a small neighborhood in the vicinity of each window" - Page 2, Introduction]

## Suggestions / Methods

- 계층적 ViT 아키텍처를 제안하여 지역 및 전역 자기 주의 모듈을 결합 ["we propose a hierarchical ViT architecture consisting of local and global self-attention modules" - Page 2, Introduction]

- 각 단계에서 수정된 퓨즈드 인버티드 레지듀얼 블록을 사용하여 전역 쿼리 토큰을 계산 ["At each stage, we compute global query tokens, using a novel fused inverted residual blocks" - Page 2, Introduction]

- 전역 쿼리 토큰은 모든 전역 자기 주의 모듈에서 공유되어 지역 키 및 값 표현과 상호작용 ["the global query tokens are shared across all global self-attention modules to interact with local key and value representations" - Page 2, Introduction]

- 추가적으로 다운샘플링 블록에 수정된 퓨즈드-MBConv 레이어를 도입 ["we propose a novel downsampling block with a parameter-efficient fused-MBConv layer" - Page 2, Introduction]


