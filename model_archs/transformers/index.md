## Paper review index for Transformer-based Archtectures

#### Language Model

| Index                                                                             | Title & Link                                                                                                 | Paper                                                                                  | Submission | Year |
| --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------- | ---------- | ---- |
| [[01. Learning to (Learn at Test Time) - RNNs with Expressive Hidden States\|01]] | [Learning to (learn at **test time**): Rnns with expressive hidden states](https://arxiv.org/abs/2407.04620) | [[01. Learning to (Learn at Test Time) - RNNs with Expressive Hidden States.pdf\|pdf]] | #Pre-Print |      |
| 02                                                                                | [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871)                | [[02. Byte Latent Transformer - Patches Scale Better Than Tokens.pdf\|pdf]]            | #Pre-Print |      |
|                                                                                   |                                                                                                              |                                                                                        |            |      |

#### Vision Model

| Index | Title & Link                                                                                                                                                                                                               | Paper                                                                                                                                          | Submission |
| ----- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ---------- |
| 03    | [YOLOv10: Real-Time End-to-End Object Detection](https://arxiv.org/abs/2405.14458)                                                                                                                                         | [[03. YOLOv10 - Real-Time End-to-End Object Detection.pdf\|pdf]]                                                                               | #NeurIPS   |
| 04    | [Evaluating the Evolution of YOLO (You Only Look Once) Models: A Comprehensive Benchmark Study of YOLO11 and Its Predecessors](https://arxiv.org/abs/2411.00201)                                                           | [[04. Evaluating the Evolution of YOLO (You Only Look Once) Models - A Comprehensive Benchmark Study of YOLO11 and Its Predecessors.pdf\|pdf]] | #Pre-Print |
| 05    | [**End**-**to**-**end object detection** with **transformers**](https://arxiv.org/abs/2005.12872)                                                                                                                          | [[05. End-to-End Object Detection with Transformers.pdf\|pdf]]                                                                                 | #EECV      |
| 06    | [Dinov2: Learning robust visual features without supervision](https://arxiv.org/abs/2304.07193)                                                                                                                            | [[06. DINOv2 - Learning Robust Visual Features without Supervision.pdf\|pdf]]                                                                  | #Pre-Print |
| 07    | [An image is worth 16x16 words: Transformers for image recognition at scale](https://arxiv.org/abs/2010.11929)                                                                                                             | [[07. An image is worth 16x16 words - Transformers for image recognition at scale.pdf\|pdf]]                                                   | #Pre-Print |
| 08    | [Swin Transformer - Hierarchical Vision Transformer Using Shifted Windows](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper) | [[08. Swin Transformer - Hierarchical Vision Transformer Using Shifted Windows.pdf\|pdf]]                                                      | #ICCV      |
| 09    | [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)                                                                                                     | [[09. SegFormer - Simple and Efficient Design for Semantic Segmentation with Transformers.pdf\|pdf]]                                           | #NeurIPS   |
|       |                                                                                                                                                                                                                            |                                                                                                                                                |            |
|       |                                                                                                                                                                                                                            |                                                                                                                                                |            |
|       |                                                                                                                                                                                                                            |                                                                                                                                                |            |
![[segformer_eval.png]]

#### Linear Attention

| Index | Title & Link       | Paper | Submission |
| ----- | ------------------ | ----- | ---------- |
|       | Linformer          |       |            |
|       | Linear Transformer |       |            |
|       | Performer          |       |            |
|       |                    |       |            |

