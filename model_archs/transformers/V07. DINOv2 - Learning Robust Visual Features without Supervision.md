---
author:
  - A
  - B
submission:
  - PMIR
  - B
year: "2024"
file: 
related: 
tags:
  - Test-Time-Learning
review date: 2025-01-03
---
# <font color="#31859b">Summary</font>

## 1-Line

```
DINOv2는 대규모 큐레이트된 데이터셋에서 자기지도학습을 통해 다양한 태스크에서 SOTA 성능을 달성한 비전 모델
```
## 3-Line

```
자기지도학습 방식으로 1억 4천만장의 큐레이트된 이미지로 학습한 범용 비전 모델
```

```
기존 기법들을 개선하고 결합하여 데이터와 모델 크기 측면에서의 확장성을 높임
```

```
이미지/픽셀 수준의 다양한 벤치마크에서 기존 최고 성능을 뛰어넘는 결과
```
## 5-Line

```
142M 규모의 큐레이트된 데이터셋(LVD-142M)을 구축하여 학습에 활용
```

```
DINO와 iBOT의 손실함수를 결합하고 다양한 기술적 개선을 통해 학습 안정성과 성능을 높임
```

```
1B 파라미터 규모의 ViT 모델을 학습하고 이를 작은 모델들로 증류
```

```
다양한 downstream 태스크에서 기존 자기지도학습 모델들의 성능을 크게 개선
```

```
일부 태스크에서는 약지도학습 기반의 OpenCLIP과 같은 최고 성능 모델들도 뛰어넘었음
```


# <font color="#7030a0">Paper Review</font>

## Abstract / Motivation

> "These models could greatly simplify the use of images in any system by producing general purpose visual features, i.e., features that work across image distributions and tasks without finetuning."

DINOv2는 자연어처리 분야에서의 foundation model과 같이, 컴퓨터 비전 분야에서도 finetuning 없이 바로 사용 가능한 범용 visual feature를 학습하는 것을 목표로 함

주요 motivation은 다음과 같습니다:

1. 기존 자기지도학습 방식들이 충분한 양의 큐레이트된 데이터로 학습된다면 강력한 visual feature를 학습할 수 있다는 것을 보여주고자 했습니다 ["This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources"] [p.1].
2. 텍스트 기반의 지도 학습은 이미지의 풍부한 정보를 모두 포착하기 어렵다는 한계가 있습니다 ["This form of text-guided pretraining limits the information that can be retained about the image since captions only approximate the rich information in images"] [p.2].
## Background

자기지도학습 기반의 vision model들은 다음과 같은 장단점이 있었습니다:

1. ImageNet-1k와 같은 작은 데이터셋에서는 좋은 성능을 보여왔습니다 ["most of the advances in self-supervised learning were made in the context of pretraining on a small curated dataset, ImageNet-1k"] [p.2].
2. 하지만 큰 규모의 uncurated 데이터셋으로 확장했을 때는 feature 품질이 크게 저하되었습니다 ["Some efforts on scaling these approaches beyond ImageNet-1k have been attempted but they focused on uncurated datasets, which typically lead to a significant drop in the quality of the features"] [p.2].

## Targeting Problems

- 자기지도학습 모델의 학습 안정성 문제
    - 큰 규모의 데이터와 모델로 확장할 때 학습이 불안정해지는 문제가 있었습니다.
- 계산 효율성 문제
    - 기존 방식들은 메모리와 계산량 측면에서 비효율적이었습니다. ["These improvements make our approach around 2× faster and require 3× less memory than similar discriminative self-supervised methods"] [p.2].
- 데이터 품질과 다양성 문제
    - Uncurated 데이터셋의 낮은 품질과 편향된 분포가 성능을 저하시켰습니다.

## Suggestions / Methods

주요 제안 방법은 다음과 같습니다:

1. 학습 안정성 개선
    - LayerScale, stochastic depth 등을 도입했습니다.
    - Sinkhorn-Knopp centering을 적용했습니다.
    - KoLeo regularizer를 추가했습니다. ["Our approach improves over the iBOT method by combining it with several existing components described in Sec. 4"] [p.7].
2. 계산 효율성 향상
    - Sequence packing을 통해 계산을 최적화했습니다.
    - 효율적인 stochastic depth 구현을 도입했습니다.
    - Fully-Sharded Data Parallel 방식을 사용했습니다. ["Most of our technical improvements to the training loop aim at improving the training of large models over large quantities of data"] [p.7].
3. 데이터셋 구축
    - 142M 규모의 큐레이트된 데이터셋을 구축
    - 자동화된 필터링과 rebalancing 파이프라인을 구축 ["We have built an automatic pipeline to filter and rebalance datasets from an extensive collection of uncurated images"] [p.2].


