---
author:
  - A
  - B
submission:
  - PMIR
  - B
year: "2024"
file: 
related: 
tags: []
review date: 2025-01-03
---
# <font color="#31859b">Summary</font>

## 1-Line

>"We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components"

```
DETR(DEtection TRansformer)은 객체 탐지를 직접적인 집합 예측 문제로 접근하여 기존의 수작업 설계 컴포넌트들을 제거한 end-to-end 객체 탐지 모델 [p.1 Introduction]
```
## 3-Line

>"The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture."

```
DETR은 bipartite matching loss와 transformer encoder-decoder 구조를 통해 이미지에서 객체들을 직접 예측 [p.1 Abstract]
```

>"Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation"

```
NMS(Non-Maximum Suppression)나 anchor box 생성과 같은 수작업 설계 요소들을 제거하여 파이프라인을 단순화 [p.1 Abstract]
```

>"DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset."

```
COCO 데이터셋에서 Faster R-CNN과 동등한 성능을 보이면서도 panoptic segmentation으로의 확장이 용이 [p.1 Abstract]
```
## 5-Line

>"Modern detectors address this set prediction task in an indirect way, by defining surrogate regression and classification problems on a large set of proposals"

```
기존 객체 탐지 방법들은 surrogate tasks를 통한 간접적인 접근법을 사용했지만, DETR은 직접적인 set prediction으로 문제를 해결 [p.1 Introduction]
```

>"The self-attention mechanisms of transformers, which explicitly model all pairwise interactions between elements in a sequence"

```
Transformer의 self-attention 메커니즘을 활용하여 이미지 전체의 문맥을 고려하고 객체들 간의 관계를 모델링 [p.2 Section 2]
```

>"Our matching loss function uniquely assigns a prediction to a ground truth object"

```
Hungarian algorithm을 사용한 bipartite matching loss로 예측과 ground truth 객체 간의 unique matching을 보장 [p.5 Section 3.1]
```

>"Unlike many other modern detectors, DETR doesn't require any customized layers, and thus can be reproduced easily in any framework that contains standard CNN and transformer classes"

```
CNN backbone과 transformer encoder-decoder를 결합한 간단한 구조로, 특별한 라이브러리 없이도 구현이 가능 [p.6 Section 3.2]
```

>"DETR demonstrates significantly better performance on large objects... It obtains, however, lower performances on small objects"

```
대형 객체에서는 Faster R-CNN보다 우수한 성능을 보이지만, 소형 객체에서는 아직 개선의 여지가 있음 [p.2 Section 2]
```


# <font color="#7030a0">Paper Review</font>

## Abstract / Motivation

> "Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task."

- DETR의 주요 동기는 기존 객체 탐지 시스템의 복잡성과 수작업 설계 요소들을 제거하는 것
- 특히 NMS나 anchor box와 같은 사전 지식 기반의 컴포넌트들을 제거하고, end-to-end 학습이 가능한 단순하고 직관적인 시스템을 만드는 것이 목표 [p.1 Abstract]
## Background

1. 기존의 객체 탐지 시스템들은 다음과 같은 접근 방식을 사용:

- Proposal 기반 2-stage 탐지기 [p.4 Section 2.3]
	- 원문: "Two-stage detectors predict boxes w.r.t. proposals"
- Anchor 기반 single-stage 탐지기 [p.4 Section 2.3]
	- 원문: "single-stage methods make predictions w.r.t. anchors or a grid of possible object centers"

2. Set Prediction 관련 이전 연구들:

- 멀티라벨 분류에서는 one-vs-rest 방식이 사용되었으나, 객체 탐지에서는 box들 간의 유사성 때문에 적용이 어려웠음 [p.3 Section 2.1]
	- 원문: "The basic set prediction task is multilabel classification for which the baseline approach, one-vs-rest, does not apply to problems such as detection where there is an underlying structure between elements"

- 중복 예측을 피하기 위해 NMS와 같은 후처리가 필요 [p.3 Section 2.1]
	- 원문: "Most current detectors use postprocessings such as non-maximal suppression to address this issue"

## Targeting Problems

- 복잡한 파이프라인과 수작업 설계 요소들의 제거 [p.1 Abstract]
	- 원문: "removing the need for many hand-designed components that explicitly encode our prior knowledge about the task"

- 중복 예측 문제 해결 [p.3 Section 2.1]
	- 원문: "The first difficulty in these tasks is to avoid near-duplicates"

- End-to-end 학습의 어려움 [p.1 Introduction]
	- 원문: "This end-to-end philosophy has led to significant advances in complex structured prediction tasks such as machine translation or speech recognition, but not yet in object detection"

## Suggestions / Methods

##### 구조 정리

1. Transformer 구조 활용
	- CNN backbone으로 이미지 특징 추출 [p.6 Section 3.2]
		- 원문: "Starting from the initial image ximg ∈ R3×H0×W0, a conventional CNN backbone generates a lower-resolution activation map"

	- Encoder-decoder transformer로 global attention 수행 [p.2 Section 2]
		- 원문: "We adopt an encoder-decoder architecture based on transformers, a popular architecture for sequence prediction"

2. Bipartite Matching Loss
	- Hungarian algorithm을 통한 예측-ground truth 매칭 [p.5 Section 3.1]
		- 원문: "This optimal assignment is computed efficiently with the Hungarian algorithm"

	- Class 예측과 box 위치에 대한 결합된 손실 함수 [p.5 Section 3.1]
		- 원문: "The matching cost takes into account both the class prediction and the similarity of predicted and ground truth boxes"

3. Parallel Decoding
	- N개의 object queries를 동시에 처리 [p.7 Section 3.2]
		- 원문: "The decoder follows the standard architecture of the transformer, transforming N embeddings of size d using multi-headed self- and encoder-decoder attention mechanisms"

##### 상세 분석

###### CNN Backbone
- **ResNet 모델을 backbone으로 사용** [p.8 Section 4]
	- 원문: "All transformer weights are initialized with Xavier init, and the backbone is with ImageNet-pretrained ResNet model from torchvision with frozen batchnorm layers"
- 마지막 분류 레이어를 제거하고 특징맵 추출 [p.6 Section 3.2]
	- 원문: "Starting from the initial image ximg ∈ R3×H0×W0 (with 3 color channels), a conventional CNN backbone generates a lower-resolution activation map f ∈ RC×H×W"
###### Transformer Encoder
1. 입력 처리:
	- 1x1 convolution으로 채널 차원 축소 [p.6 Section 3.2]
		- 원문: "First, a 1x1 convolution reduces the channel dimension of the high-level activation map f from C to a smaller dimension d"
	- Spatial dimensions을 sequence로 변환 [p.6 Section 3.2]
		- 원문: "The encoder expects a sequence as input, hence we collapse the spatial dimensions of z0 into one dimension"

2. Position Encoding:
	- **Fixed positional encodings 사용** [p.6 Section 3.2]
		- 원문: "Since the transformer architecture is permutation-invariant, we supplement it with fixed positional encodings that are added to the input of each attention layer"
##### Transformer Decoder
1. Object Queries:
	- N개의 learned positional encodings 사용 [p.7 Section 3.2]
		- 원문: "These input embeddings are learnt positional encodings that we refer to as object queries"

2. Cross-Attention:
	- **Encoder 출력과 object queries 간의 attention** [p.7 Section 3.2]
		- 원문: "Using self- and encoder-decoder attention over these embeddings, the model globally reasons about all objects together using pair-wise relations between them"
###### Prediction Heads
1. FFN 구조:
	- 3-layer perceptron과 ReLU activation 사용 [p.7 Section 3.2]
		- 원문: "The final prediction is computed by a 3-layer perceptron with ReLU activation function and hidden dimension d"

2. 출력:
	- 클래스 예측을 위한 softmax layer [p.7 Section 3.2]
		- 원문: "and the linear layer predicts the class label using a softmax function"

	- Box 좌표 예측 [p.7 Section 3.2]
		- 원문: "The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image"
###### Training Details
1. 최적화 설정:
	- AdamW optimizer 사용 [p.8 Section 4]
		- 원문: "We train DETR with AdamW setting the initial transformer's learning rate to 10−4, the backbone's to 10−5, and weight decay to 10−4"

2. Data Augmentation:
	- Scale augmentation 적용 [p.8 Section 4]
		- 원문: "We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333"

3. Training Schedule:
	- 긴 학습 기간 필요 [p.8 Section 4]
		- 원문: "Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days, with 4 images per GPU"
###### Limitations
1. 작은 객체 탐지 성능:
	- 소형 객체에서 상대적으로 낮은 성능 [p.2 Section 2]
		- 원문: "It obtains, however, lower performances on small objects. We expect that future work will improve this aspect"
		- 개선:
			- Anchor DETR (2022): DETR의 anchor-free 방식 대신 anchor 개념을 재도입하여 성능이 크게 개선됨
			- DAB-DETR (2022): Query를 4D reference points로 보는 접근법 도입하여 더 정확한 localization 달성
			- DN-DETR (2022): Denoising Training 전략 도입
			- 그러나 여전히 소형 객체 탐지에서 기존의 anchor 기반 방식인 Faster R-CNN과 같은 모델이 강점을 보이는 경우가 많아 transformer 기반 탐지기의 소형 객체 탐지 성능이 여전히 challenge가 되고 있음
				- 1. Attention의 global nature로 인한 디테일 손실
				- 2. Feature pyramid 구조의 부재
				- 3. 고해상도 feature map 처리 시의 계산 비용 문제

2. 학습 시간:
	- 긴 학습 시간 필요 [p.2 Section 2]
		- 원문: "Training settings for DETR differ from standard object detectors in multiple ways. The new model requires extra-long training schedule"