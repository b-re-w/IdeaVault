---
author:
  - Yu Sun
  - Xinhao Li
  - Karan Dalal
  - Jiarui Xu
  - Arjun Vikram
  - Genghan Zhang
  - Yann Dubois
  - Xinlei Chen
  - Xiaolong Wang
  - Sanmi Koyejo
  - Tatsunori Hashimoto
  - Carlos Guestrin
submission: 
year: "2024"
file:
  - "[[01. Learning to (Learn at Test Time) - RNNs with Expressive Hidden States.pdf|05. Learning to (Learn at Test Time) - RNNs with Expressive Hidden States]]"
related: 
tags:
  - Test-Time-Learning
  - Test-Time-Training
review date: 2024-11-12
---
# <font color="#31859b">Summary</font>

## 1-Line

```
Test-Time Training (TTT) 레이어는 선형 복잡도를 가지면서도 긴 컨텍스트에서 효과적인 성능을 보이는 새로운 시퀀스 모델링 레이어를 제안 [Abstract, p.1]
```
## 3-Line

```
Self-attention은 긴 컨텍스트에서 좋은 성능을 보이지만 이차 복잡도를 가지고, 기존 RNN은 선형 복잡도를 가지지만 긴 컨텍스트에서 성능이 제한적 [Section 1, p.2]
```

```
TTT 레이어는 히든 스테이트를 머신러닝 모델로 구성하고 업데이트 규칙을 자기지도학습 단계로 설정하여 선형 복잡도와 표현력 있는 히든 스테이트를 동시에 달성 [Section 1, p.2-3]
```

```
TTT-Linear와 TTT-MLP 두 가지 구현을 제안했으며, Transformer와 Mamba와 비교했을 때 125M에서 1.3B 파라미터 규모에서 동등하거나 더 좋은 성능을 보임 [Abstract, p.1]
```
## 5-Line

```
Self-attention과 기존 RNN의 장단점을 분석하고 긴 컨텍스트에서 효과적인 새로운 아키텍처의 필요성을 제기 [Section 1, p.2]
```

```
히든 스테이트를 머신러닝 모델로 구성하고 업데이트를 자기지도학습으로 하는 Test-Time Training (TTT) 레이어를 제안 [Section 2.1, p.3-4]
```

```
Mini-batch TTT와 dual form을 통해 하드웨어 효율성을 개선 [Section 2.4-2.5, p.7-8]
```

```
TTT-Linear와 TTT-MLP 두 가지 구현을 제안하고 각각의 특성을 분석 [Section 2.6-2.7, p.10-12]
```

```
실험을 통해 제안한 방법이 긴 컨텍스트에서 기존 방법들보다 우수한 성능을 보임을 일부 입증 [Section 3, p.13-17]
```


# <font color="#7030a0">Paper Review</font>

## Abstract / Motivation

> 기존 RNN은 선형 복잡도를 가지지만 히든 스테이트의 표현력 한계로 긴 컨텍스트에서 성능이 제한적이다. ["Self-attention can also be viewed from the perspective above, except that its hidden state, commonly known as the Key-Value (KV) cache, is a list that grows linearly with t.", Section 1, p.2]

- Self-attention은 긴 컨텍스트에서 좋은 성능을 보이지만 이차 복잡도를 가진다. ["Unlike self-attention, RNN layers have to compress context into a hidden state of fixed size.", Section 1, p.2]
	- Self-attention 메커니즘에서 이차 복잡도가 발생하는 이유는, 각 단어가 시퀀스 내의 모든 다른 단어와 상호작용하기 때문 O(n^2)
	- 이는 긴 시퀀스를 처리할 때 계산 비용과 메모리 사용량이 급격히 증가하는 원인

- 선형 복잡도를 유지하면서 표현력 있는 히든 스테이트를 가진 새로운 시퀀스 모델링 레이어가 필요 ["To remain both efficient and expressive in long context, we need a better compression heuristic.", Section 1, p.2]

## Background

- RNN 레이어들은 고정된 크기의 히든 스테이트에 컨텍스트를 압축해야 함 ["All sequence modeling layers can be viewed from the perspective of storing historic context into a hidden state", Section 2, p.3]

- Self-attention은 Key-Value 캐시를 통해 모든 히든 컨텍스트를 명시적으로 저장 ["The hidden state explicitly stores all historic context without compression, making self-attention more expressive than RNN layers for long context.", Section 2, p.4]

- 자기지도학습은 대규모 훈련 데이터를 모델 가중치로 효과적으로 압축 가능 ["The process of parametric learning can be viewed as compressing a massive training set into the weights of a model.", Section 2.1, p.4]

## Targetting Problems

- 기존 RNN의 히든 스테이트 표현력 한계 극복 [Section 1, p.2]

- Self-attention의 이차 복잡도 문제 해결 [Section 1, p.2]

- 효율적인 하드웨어 활용을 위한 최적화 필요 [Section 2.5, p.7-8]

## Suggestions / Methods

#### 1. TTT Layer 설계:

- 히든 스테이트를 머신러닝 모델로 구성 ["Our key idea is to make the hidden state itself a model f with weights W", Section 2.1, p.4]

	- TTT Layer는 히든 스테이트를 머신러닝 모델(f)로 구성하고, 파라미터(W)를 통해 컨텍스트 정보를 저장함 ["The hidden state st is now equivalent to Wt, the weights of a model f", Section 2.1, p.4]

	- 모든 시퀀스 데이터에 대해 테스트 타임에 학습이 이뤄짐으로써 각 시퀀스에 맞는 최적의 파라미터를 찾음 ["Even at test time, our new layer still trains a different sequence of weights W1,...,WT for every input sequence", Section 2.1, p.5]
	
	- Inner loop(TTT)와 Outer loop(네트워크 학습)의 이중 학습 구조를 가짐 ["We refer to training the larger network as the outer loop, and training W within each TTT layer as the inner loop", Section 2.2, p.6]

- 업데이트 규칙을 자기지도학습 단계로 설정 ["and the update rule a step of self-supervised learning", Section 2.1, p.4]

	- 입력 토큰 xt를 corrupted input x̃t로 변환하고 이를 다시 복원하는 task를 학습 ["One choice of ℓ is reconstructing xt itself. To make the learning problem nontrivial, we first process xt into a corrupted input x̃t", Section 2.3, p.6-7]

	- θK, θV, θQ 세 개의 학습 가능한 projection matrices를 도입:
		- Input Token xt가 들어오면 3가지 뷰로 투영되는데, 이 중 Model Weights (Wt)는 Training view와 Label view를 사용해 self-supervised learning으로 업데이트 / 최종 Output (zt) 생성은 업데이트된 weight로 Test View를 처리하여 진행
		- <svg viewBox="0 0 800 500" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
    </marker>
  </defs>

  <!-- Input token -->
  <rect x="50" y="230" width="100" height="40" fill="#E3F2FD" stroke="#1565C0"/>
  <text x="100" y="255" text-anchor="middle" font-size="14">Input Token xt</text>

  <!-- Linear projections - Reordered Q,K,V -->
  <rect x="250" y="150" width="100" height="40" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="300" y="175" text-anchor="middle" font-size="14">θQ (Linear)</text>

  <rect x="250" y="230" width="100" height="40" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="300" y="255" text-anchor="middle" font-size="14">θK (Linear)</text>

  <rect x="250" y="310" width="100" height="40" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="300" y="335" text-anchor="middle" font-size="14">θV (Linear)</text>

  <!-- Views - Reordered -->
  <rect x="450" y="150" width="100" height="40" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="500" y="175" text-anchor="middle" font-size="14">Test View</text>

  <rect x="450" y="230" width="100" height="40" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="500" y="255" text-anchor="middle" font-size="14">Training View</text>

  <rect x="450" y="310" width="100" height="40" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="500" y="335" text-anchor="middle" font-size="14">Label View</text>

  <!-- Model box -->
  <rect x="200" y="50" width="500" height="350" fill="none" stroke="#9C27B0" stroke-width="2" stroke-dasharray="5,5"/>
  <text x="450" y="80" text-anchor="middle" font-size="16" fill="#9C27B0">Model Weights (Wt)</text>

  <!-- Update box -->
  <rect x="300" y="400" width="300" height="60" fill="#E1BEE7" stroke="#6A1B9A"/>
  <text x="450" y="430" text-anchor="middle" font-size="14">Wt = Wt-1 - η∇L(Wt-1)</text>

  <!-- Output -->
  <rect x="650" y="150" width="100" height="40" fill="#F3E5F5" stroke="#6A1B9A"/>
  <text x="700" y="175" text-anchor="middle" font-size="14">zt = f(Test View)</text>

  <!-- Simple straight arrows -->
  <line x1="150" y1="250" x2="250" y2="170" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="150" y1="250" x2="250" y2="250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="150" y1="250" x2="250" y2="330" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <line x1="350" y1="170" x2="450" y2="170" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="350" y1="250" x2="450" y2="250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="350" y1="330" x2="450" y2="330" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>

  <!-- Output arrow -->
  <line x1="550" y1="170" x2="650" y2="170" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
</svg>
	    - θK: training view 생성 (입력 변환)
		    - 모델이 현재 입력 토큰의 어떤 특징을 학습할 지 결정 (QK로 만들어진 입력 토큰의 저차원 투영)
		    - 자기 지도학습 로스 계산시 입력으로 사용됨
		    - f(Training view)가 Label view를 잘 복원하도록 학습
		    - 부분 정보(Training view)로부터 전체 정보(Label view) 복원
	    - θV: label view 생성 (출력 레이블)
		    - 모델이 실제로 학습해야 할 목표값 제공
		    - 자기 지도학습 로스 계산시 타겟으로 사용됨
		- <svg viewBox="0 0 800 400" xmlns="http://www.w3.org/2000/svg">
  <!-- Input Token -->
  <rect x="50" y="170" width="120" height="60" fill="#E3F2FD" stroke="#1565C0"/>
  <text x="110" y="205" text-anchor="middle" font-size="14">Input Token xt</text>

  <!-- K Processing -->
  <rect x="250" y="100" width="120" height="60" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="310" y="130" text-anchor="middle" font-size="14">θK Projects</text>
  <text x="310" y="150" text-anchor="middle" font-size="12">Dimensional</text>
  <text x="310" y="165" text-anchor="middle" font-size="12">Reduction</text>

  <!-- V Processing -->
  <rect x="250" y="240" width="120" height="60" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="310" y="270" text-anchor="middle" font-size="14">θV Projects</text>
  <text x="310" y="290" text-anchor="middle" font-size="12">Target</text>
  <text x="310" y="305" text-anchor="middle" font-size="12">Features</text>

  <!-- Training View -->
  <rect x="450" y="100" width="120" height="60" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="510" y="135" text-anchor="middle" font-size="14">Training View</text>
  <text x="510" y="155" text-anchor="middle" font-size="12">(Partial Info)</text>

  <!-- Label View -->
  <rect x="450" y="240" width="120" height="60" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="510" y="275" text-anchor="middle" font-size="14">Label View</text>
  <text x="510" y="295" text-anchor="middle" font-size="12">(Target Info)</text>

  <!-- Reconstruction Goal -->
  <rect x="650" y="170" width="120" height="60" fill="#F3E5F5" stroke="#6A1B9A"/>
  <text x="710" y="195" text-anchor="middle" font-size="14">Reconstruction</text>
  <text x="710" y="215" text-anchor="middle" font-size="12">f(Training View)</text>
  <text x="710" y="230" text-anchor="middle" font-size="12">≈ Label View</text>

  <!-- Arrows -->
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
    </marker>
  </defs>
  
  <line x1="170" y1="200" x2="250" y2="130" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="170" y1="200" x2="250" y2="270" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="370" y1="130" x2="450" y2="130" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="370" y1="270" x2="450" y2="270" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="570" y1="130" x2="650" y2="190" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="570" y1="270" x2="650" y2="210" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
</svg>
	    - θQ: test view 생성 (출력 생성) ["Following the terminology of multi-view reconstruction, θKxt is called a training view", Section 2.3, p.7]
	    - <svg viewBox="0 0 800 500" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333"/>
    </marker>
  </defs>

  <!-- Input Token -->
  <rect x="50" y="220" width="120" height="60" fill="#E3F2FD" stroke="#1565C0"/>
  <text x="110" y="255" text-anchor="middle" font-size="14">Input Token xt</text>

  <!-- Initial Processing -->
  <rect x="250" y="150" width="120" height="60" fill="#E8F5E9" stroke="#2E7D32"/>
  <text x="310" y="180" text-anchor="middle" font-size="14">θK Projects</text>
  <text x="310" y="200" text-anchor="middle" font-size="12">Training View</text>

  <rect x="250" y="290" width="120" height="60" fill="#FFF3E0" stroke="#EF6C00"/>
  <text x="310" y="320" text-anchor="middle" font-size="14">θV Projects</text>
  <text x="310" y="340" text-anchor="middle" font-size="12">Label View</text>

  <!-- Loss and Update -->
  <rect x="450" y="220" width="120" height="60" fill="#F5F5F5" stroke="#424242"/>
  <text x="510" y="245" text-anchor="middle" font-size="14">1. Loss (L)</text>
  <text x="510" y="265" text-anchor="middle" font-size="12">||f(Training View)</text>
  <text x="510" y="280" text-anchor="middle" font-size="12">- Label View||²</text>

  <rect x="450" y="320" width="120" height="60" fill="#E1BEE7" stroke="#6A1B9A"/>
  <text x="510" y="345" text-anchor="middle" font-size="14">2. Update</text>
  <text x="510" y="365" text-anchor="middle" font-size="12">Wt = Wt-1 - η∇L</text>

  <!-- Output Generation -->
  <rect x="650" y="220" width="120" height="60" fill="#F3E5F5" stroke="#6A1B9A"/>
  <text x="710" y="245" text-anchor="middle" font-size="14">3. Generate zt</text>
  <text x="710" y="265" text-anchor="middle" font-size="12">using updated Wt</text>

  <!-- Model Box -->
  <rect x="200" y="80" width="500" height="350" fill="none" stroke="#9C27B0" stroke-width="2" stroke-dasharray="5,5"/>
  <text x="400" y="110" text-anchor="middle" font-size="16" fill="#9C27B0">Model Weights Update Process</text>

  <!-- Arrows -->
  <line x1="170" y1="250" x2="250" y2="180" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="170" y1="250" x2="250" y2="320" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="370" y1="180" x2="450" y2="250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="370" y1="320" x2="450" y2="250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <line x1="510" y1="280" x2="510" y2="320" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  <line x1="570" y1="350" x2="650" y2="250" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
</svg>

- 출력함수 f에 대해 TTT-Linear와 TTT-MLP 두 가지 구현 제안 ["We propose two variants of TTT layers – TTT-Linear and TTT-MLP", Section 2.7, p.12]

	- TTT-Linear: 히든 스테이트가 선형 모델(f(x) = Wx)로 구성됨

	- TTT-MLP: 히든 스테이트가 2층 MLP로 구성됨, 더 표현력이 높음 (2-layer MLP with GELU activation)

	- 둘 다 Layer Normalization과 residual connection을 포함 ["For TTT-Linear, flin(x) = Wx, where W is square. For TTT-MLP, fMLP has two layers similar to the MLPs in Transformers", Section 2.7, p.12]

#### 효율성 개선:

- Mini-batch TTT를 통한 병렬화 ["Our proposed solution – mini-batch gradient descent", Section 2.4, p.7]

	- 기존 online gradient descent는 순차적 의존성으로 인해 병렬화가 어려움
	- Mini-batch TTT는 batch size b만큼의 토큰을 모아서 한 번에 그래디언트 계산
	- 각 미니배치 내의 그래디언트 계산이 독립적이므로 병렬 처리 가능 ["To parallelize Gt for t = 1,...,T, we can take all of them w.r.t. W0... Our proposed solution – mini-batch gradient descent", Section 2.4, p.7-8]

- Dual form을 통한 하드웨어 효율성 향상 ["We call this procedure the dual form, in contrast to the primal form", Section 2.5, p.7-8]

	- 하드웨어 효율성을 위해 중간 변수들(Gs, Ws)를 실제로 계산하지 않음
	- Matrix multiplication을 주로 사용하여 최종 출력(Wb, z1,...,zb)만 직접 계산
	- 현대 하드웨어(GPU, TPU)의 matrix multiplication 최적화 활용 ["The goal of the dual form is to compute z̄XK+1 and W1b,...,WKb with only matmuls and light-weight operations such as sums, σ, and σ'", Section 2.5, p.8]

	* ***dual form은 primal form에 비해:
		- 이론적 복잡도는 약간 높지만(O(b×d²) vs O(b²×d))
		- 하드웨어 활용도가 높아 실제로는 5배 이상 빠름 ["In our JAX implementation, training with the dual form is more than 5× faster than with primal", Section 2.5, p.9]

- 메모리 효율적인 그래디언트 체크포인팅 ["A standard technique to save memory in this scenario is gradient checkpointing", Appendix C, p.31]

	- 일반적으로 JAX나 PyTorch같은 라이브러리들은 backward pass를 위해 forward pass의 중간 결과(activation)들을 모두 저장함 ["By default, libraries such as JAX and PyTorch save the intermediate activations during a forward pass so they can be reused during the backward pass", Appendix C, p.31]
	- TTT layer의 경우 히든 스테이트 W1,...,WT를 모두 저장해야 하므로 메모리 사용량이 매우 큼

	##### 해결 방법:

	1. 시간 축으로의 체크포인팅:
		- Mini-batch TTT에서는 κ = T/b 개의 체크포인트만 저장 (b는 batch size)
		- 각 미니배치의 마지막 W만 저장하고 나머지는 필요할 때 다시 계산 ["With TTT mini-batch and the dual form, we still need to save (assume divisible) κ = T/b Ws at the end of the mini-batches", Appendix C, p.31]

	1. 메모리-계산 트레이드오프:
		- 메모리 사용량은 줄어들지만 일부 계산을 다시 해야 함
		- 기존의 layer 방향 체크포인팅과 달리 시간 방향으로 적용 ["A standard technique to save memory in this scenario is gradient checkpointing, which is usually applied through layers, but we apply it through time", Appendix C, p.31]

	###### 장점:

	1. 메모리 효율성:
		- W의 전체 시퀀스 대신 체크포인트들만 저장
		- 긴 시퀀스 처리가 가능해짐
	
	2. 유연성:
		- Mini-batch size를 조절하여 메모리-속도 트레이드오프 조정 가능
		- 하드웨어 제약에 맞춰 최적화 가능
	
		*** 특히 긴 시퀀스를 처리할 때, 일반적인 layer 방향의 체크포인팅과 달리 시간 축으로 적용되어 TTT의 특성을 잘 활용할 수 있게 됨


