---
author:
  - A
  - B
submission:
  - PMIR
  - B
year: "2024"
file: 
related: 
tags: []
review date: 2025-01-03
---
# <font color="#31859b">Summary</font>

## 1-Line

>"We propose a novel way to leverage self-supervised contrastive learning to facilitate target feature learning, along with an online pseudo labeling scheme with refinement that significantly denoises pseudo labels."

```
AdaContrast는 source-free domain adaptation을 위해 contrastive learning과 refined pseudo labeling을 결합한 새로운 접근 방식을 제안 [p.1, Abstract]
```
## 3-Line

>"Our method, AdaContrast, achieves state-of-the-art performance on major benchmarks while having several desirable properties compared to existing works, including memory efficiency, insensitivity to hyper-parameters, and better model calibration."

```
Source 데이터 없이 target domain에 적응하기 위해 self-supervised contrastive learning을 활용 [p.1, Abstract]
```

```
Online pseudo labeling scheme을 통해 noise가 적은 pseudo label을 생성 [p.1, Abstract]
```

```
State-of-the-art 성능과 함께 메모리 효율성, 하이퍼파라미터 민감도 감소, 더 나은 모델 캘리브레이션 등의 장점 [p.1, Abstract]
```
## 5-Line

```
Test-time adaptation은 source 데이터 없이 target domain에 적응해야 하는 특수한 상황에 대한 것 [p.1, Introduction]
```

```
AdaContrast는 target domain의 pairwise 정보를 활용하기 위해 self-supervised contrastive learning을 도입 [p.2, Section 1]
```

```
Online pseudo label refinement를 통해 nearest neighbor soft voting으로 pseudo label의 품질을 향상 [p.3, Section 3.1]
```

```
Contrastive learning과 pseudo labeling을 joint optimization하여 상호 보완적인 효과 발생 [p.5, Section 3.2]
```

```
VisDA-C와 DomainNet-126 데이터셋에서 최고 성능을 달성하며, 특히 모델 캘리브레이션과 하이퍼파라미터 민감도 측면에서 장점 [p.6-7, Section 4.2-4.3]
```


# <font color="#7030a0">Paper Review</font>

## Abstract / Motivation

> ??

## Background

> ??

## Targeting Problems

> ??

## Suggestions / Methods

> ??


